# Breast Cancer Wisconsin - Binary Classification

## Dataset
[Dataset Link](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)

### Dataset Information
The dataset contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, describing characteristics of cell nuclei present in the image. A few images can be found [here](http://www.cs.wisc.edu/~street/images/).

The separating plane was obtained using the Multisurface Method-Tree (MSM-T) classification method, which utilizes linear programming to construct a decision tree. Relevant features were selected via exhaustive search in the space of 1-4 features and 1-3 separating planes.

Additional Variable Information:
1. ID number
2. Diagnosis (M = malignant, B = benign)
3-32. Ten real-valued features computed for each cell nucleus:
    - radius (mean of distances from center to points on the perimeter)
    - texture (standard deviation of gray-scale values)
    - perimeter
    - area
    - smoothness (local variation in radius lengths)
    - compactness (perimeter^2 / area - 1.0)
    - concavity (severity of concave portions of the contour)
    - concave points (number of concave portions of the contour)
    - symmetry 
    - fractal dimension ("coastline approximation" - 1)

## Contents

1. **Libraries to Import**
2. **Import Data**
3. **EDA (Exploratory Data Analysis)**
    1. Data Type
    2. Drop "Id" Column
    3. Missing Values
    4. Unique Values
    5. Convert "Diagnosis" values into numeric values
    6. Descriptive Statistics
    7. Target Feature Distribution
    8. Relation Between Features
        - Mean Values and Diagnosis
        - Standard Error Values and Diagnosis
        - Worst Values and Diagnosis
    9. Histograms
        - Histograms of "Radius mean" for Benign and Malignant Tumors
        - Histograms of "Perimeter mean" for Benign and Malignant Tumors
        - Histograms of "Concave points mean" for Benign and Malignant Tumors
    10. Boxplots Without Normalization
    11. Data Normalization
    12. Boxplots With Normalization
4. **Feature Selection**
    1. Features to Drop Based on EDA
    2. Data Split and Feature Correlation
    3. Random Forest Classification with Low Correlated Features
    4. Random Forest Classification using Univariate Feature Selection
    5. Random Forest Classification using Recursive Feature Elimination (RFE)
    6. Random Forest Classification with Recursive Feature Elimination and Cross-Validation (RFECV)
    7. Feature Extraction with Principal Component Analysis (PCA)
5. **Model Selection**
        - Selected Features
        - Random Forest
        - Logistic Regression
        - K Neighbors Classifier (KNN)
        - Support Vector Classifier (SVC)
        - SGD Classifier
        - Decision Tree Classifier
        - Voting Classifier
        - Ada Boost Classifier
        - Gradient Boosting Classifier
        - Stochastic Gradient Boosting (SGB)
        - Extreme Gradient Boosting
6. **Model Comparison**
